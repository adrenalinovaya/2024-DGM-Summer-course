{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZXJoDiD_x-N"
   },
   "source": [
    "# Homework3: Score matching and Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxF8ewFXn1HO"
   },
   "source": [
    "## Task 1: Theory (6pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN6DycPP2Xq2"
   },
   "source": [
    "\n",
    "### Problem 1: Gaussian Diffusion (2pt)\n",
    "\n",
    "In the course we have discussed two types of gaussian diffusions:\n",
    "- $\\mathbf{x}_t = \\mathbf{x}_0 + \\sigma_t \\cdot \\boldsymbol{\\epsilon}$ - score-based models,\n",
    "- $\\mathbf{x}_t = \\sqrt{1 - \\beta_t} \\cdot \\mathbf{x}_{t-1} + \\sqrt{\\beta_t} \\cdot \\boldsymbol{\\epsilon}$ - diffusion models.\n",
    "\n",
    "One may ask, why we do not consider the more general diffusion models. It was the idea of the paper [Variational Diffusion Models](https://arxiv.org/abs/2107.00630).\n",
    "\n",
    "Let consider the diffusion of the form\n",
    "$$\n",
    "    \\mathbf{x}_t = \\alpha_t \\cdot \\mathbf{x}_0 + \\sigma_t \\cdot \\boldsymbol{\\epsilon}, \\quad \\mathbf{x}_t \\sim q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\alpha_t \\cdot \\mathbf{x}_0, \\sigma_t^2 \\cdot \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "1) Show that if the variance of $\\mathbf{x}_t$ equals to the variance of $\\mathbf{x}_0$ then we came to the standard diffusion (in this case $\\alpha_t^2 = 1 - \\sigma_t^2$). That is why the standard diffusion is called **Variance Preserving**.\n",
    "\n",
    "2) Find the distribution $q(\\mathbf{x}_t | \\mathbf{x}_s)$ for $s < t$ (you have to derive the formulas for mean and variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbxPyzwcuinj"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n_qLVZe2Xq2"
   },
   "source": [
    "### Problem 2: Implicit score matching (2pt)\n",
    "\n",
    "We have discussed score matching task at Lecture 10. The objective of score matching is\n",
    "$$\n",
    "    \\frac{1}{2} \\mathbb{E}_{\\pi}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x}) - \\nabla_\\mathbf{x} \\log \\pi(\\mathbf{x}) \\bigr\\|^2_2 \\rightarrow \\min_{\\boldsymbol{\\theta}}.\n",
    "$$\n",
    "\n",
    "And we have already known one possible solution for this task. It is denoising score matching.\n",
    "\n",
    "Here our goal is to derive one more way to solve the initial score matching problem. It is called **implicit score matching**.\n",
    "\n",
    "Let consider 1-d case ($x \\in \\mathbb{R}$).\n",
    "Prove that\n",
    "$$\n",
    "\\frac{1}{2} \\mathbb{E}_{\\pi}\\bigl\\| s_{\\boldsymbol{\\theta}}(x) - \\nabla_x \\log \\pi(x) \\bigr\\|^2_2 = \\mathbb{E}_{\\pi}\\left[ \\frac{1}{2}s^2_{\\boldsymbol{\\theta}}(x) + \\nabla_{x} s_{\\boldsymbol{\\theta}}(x) \\right] + \\text{const}.\n",
    "$$\n",
    "\n",
    "- **Q:** Why is the expression at the right hand side better than the left one? **A:** It is better because we do not have the term with the unknown distribution $\\pi(x)$.\n",
    "\n",
    "- **Q:** Why do we not use this expression instead of denoising score matching? **A:** In this expression we have term $\\nabla_{x} s_{\\boldsymbol{\\theta}}(x) = \\nabla^2_{x} \\log p(x | \\boldsymbol{\\theta})$. And it is difficult to work with the second derivates.\n",
    "\n",
    "- **Q:** Why do we consider only 1-d case? **A:** It is very straightforward to generalize this formula to the multidimensional case, but the derivation contains much more formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ApNxWRw2vPU"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuoaAaSmtzY8"
   },
   "source": [
    "### Problem 3: KFP theorem (2pt)\n",
    "\n",
    "In Lecture 13 we have faced with 2 different formulations of Kolmogorov-Fokker-Planck theorem.\n",
    "\n",
    "1) We used the following KFP theorem when we discussed continuous-in-time NF:\n",
    "$$\n",
    "\\frac{d \\log p(\\mathbf{x}(t), t)}{d t} = - \\text{tr} \\left( \\frac{\\partial f(\\mathbf{x}, t)}{\\partial \\mathbf{x}} \\right).\n",
    "$$\n",
    "\n",
    "2) We used the following KFP theorem when we discussed SDEs:\n",
    "$$\n",
    "\\frac{\\partial p(\\mathbf{x}, t)}{\\partial t} = \\text{tr}\\left(- \\frac{\\partial}{\\partial \\mathbf{x}} \\bigl[ \\mathbf{f}(\\mathbf{x}, t) p(\\mathbf{x}, t)\\bigr] + \\frac{1}{2} g^2(t) \\frac{\\partial^2 p(\\mathbf{x}, t)}{\\partial \\mathbf{x}^2} \\right)\n",
    "$$\n",
    "\n",
    "In this task your goal is to prove that the first formulation is a special case of the more general second formulation.\n",
    "\n",
    "You have to use two facts:\n",
    "1) Continuous-in-time NF use ODE (not SDE).\n",
    "2) The derivation in the first formulation is total derivative (not partial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDjAwcZ1t1j5"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13960,
     "status": "ok",
     "timestamp": 1722176832627,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "jppIpVyZ2Xq3",
    "outputId": "67212252-7031-46c9-f7ed-00ddf766b232"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "\n",
    "COMMIT_HASH = \"b4df45506b5dc4cd055cb00facebc504fd57571a\"\n",
    "!if [ -d dgm_utils ]; then rm -Rf dgm_utils; fi\n",
    "!git clone https://github.com/r-isachenko/dgm_utils.git\n",
    "%cd dgm_utils\n",
    "!git checkout {COMMIT_HASH}\n",
    "!pip install ./\n",
    "%cd ./..\n",
    "!rm -Rf dgm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4158,
     "status": "ok",
     "timestamp": 1722176836783,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "Blg_otRk23ud"
   },
   "outputs": [],
   "source": [
    "import dgm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1722176990038,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "7Snskjc-2_o4"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehGykW1bnNbo"
   },
   "source": [
    "## Task 2: Denoising score matching for 2D data (6 pts)\n",
    "\n",
    "In this task you will implement the denoising score matching model to the 2D moons dataset.\n",
    "\n",
    "Let's take a look at dataset samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1722176837671,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "P2zCpepU3Bvw"
   },
   "outputs": [],
   "source": [
    "def generate_moons_data(count: int) -> tuple:\n",
    "    data, labels = make_moons(n_samples=count, noise=0.1)\n",
    "    data = data.astype(\"float32\")\n",
    "    split = int(0.8 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    train_labels, test_labels = labels[:split], labels[split:]\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 1109,
     "status": "ok",
     "timestamp": 1722176838778,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "bJUHA3sy3Etb",
    "outputId": "e83bf1dd-6643-4b03-fdd7-e010e2c9d655"
   },
   "outputs": [],
   "source": [
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "dgm_utils.visualize_2d_data(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07H5Put_nbTI"
   },
   "source": [
    "Let recall the theory of denoising score matching.\n",
    "\n",
    "The idea is the following. We define the score function\n",
    "$$\n",
    "    \\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}| \\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "Then we minimize the Fisher divergence to obtain the score function:\n",
    "$$\n",
    "    D_F(\\pi, p) = \\frac{1}{2}\\mathbb{E}_{\\pi}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x}) - \\nabla_{\\mathbf{x}} \\log \\pi(\\mathbf{x}) \\bigr\\|^2_2 \\rightarrow \\min_{\\boldsymbol{\\theta}}\n",
    "$$.\n",
    "\n",
    "If we have the score function, we use the Langevin dynamics to sample from our model:\n",
    "$$\n",
    "    \\mathbf{x}_{l + 1} = \\mathbf{x}_l + \\frac{\\eta}{2} \\cdot \\nabla_{\\mathbf{x}_l} \\log p(\\mathbf{x}_l | \\boldsymbol{\\theta}) + \\sqrt{\\eta} \\cdot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "But Fisher divergence is intractable and we use the noising procedure to get noised samples $\\mathbf{x}_{\\sigma} = \\mathbf{x} + \\sigma \\cdot \\boldsymbol{\\epsilon}$.\n",
    "\n",
    "Minimizing the Fisher divergence for the noisy samples is equivalent to the following objective:\n",
    "\\begin{multline*}\n",
    "    \\mathbb{E}_{q(\\mathbf{x}_{\\sigma})}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}, \\sigma}(\\mathbf{x}_{\\sigma}) - \\nabla_{\\mathbf{x}_{\\sigma}} \\log q(\\mathbf{x}_{\\sigma}) \\bigr\\|^2_2 = \\\\\n",
    "    = \\mathbb{E}_{\\pi(\\mathbf{x})} \\mathbb{E}_{q(\\mathbf{x}_{\\sigma} | \\mathbf{x})}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}, \\sigma}(\\mathbf{x}_{\\sigma}) - \\nabla_{\\mathbf{x}_{\\sigma}} \\log q(\\mathbf{x}_{\\sigma} | \\mathbf{x}) \\bigr\\|^2_2 + \\text{const}(\\boldsymbol{\\theta}).\n",
    "\\end{multline*}\n",
    "\n",
    "Here\n",
    "$$\n",
    "    \\log q(\\mathbf{x}_{\\sigma} | \\mathbf{x}) = - \\frac{\\mathbf{x}_{\\sigma} - \\mathbf{x}}{\\sigma^2} = - \\frac{\\boldsymbol{\\epsilon}}{\\sigma}.\n",
    "$$\n",
    "\n",
    "Therefore, the objective of the denoising score matching is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\pi(\\mathbf{x})} \\mathbb{E}_{q(\\mathbf{x}_{\\sigma} | \\mathbf{x})}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}, \\sigma}(\\mathbf{x}_{\\sigma}) + \\frac{\\boldsymbol{\\epsilon}}{\\sigma} \\bigr\\|^2_2 \\rightarrow \\min_{\\boldsymbol{\\theta}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1722176838779,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "0ikuwxWM3Nba"
   },
   "outputs": [],
   "source": [
    "class DenoisingScoreMatcher(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            score_model: nn.Module,\n",
    "            input_shape: Tuple[int],\n",
    "            sigma: float\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.score_model = score_model\n",
    "        self.input_shape = input_shape\n",
    "        self.sigma = sigma\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # ====\n",
    "        # your code\n",
    "        # sample gaussian noise\n",
    "        # perturb samples using the noise and sigma\n",
    "        \n",
    "        noisy_x = \n",
    "        # =====\n",
    "\n",
    "        # calculate the score model\n",
    "        s = self.score_model(noisy_x)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # compute the loss\n",
    "        # it is mse between score function and gradient of the normal distribution\n",
    "        loss = \n",
    "        # =====\n",
    "        return loss\n",
    "\n",
    "    def loss(self, x: torch.Tensor):\n",
    "        return {\"total_loss\": self(x).mean(dim=0).sum()}\n",
    "\n",
    "    def langevin_dynamics(self, x: torch.Tensor, num_steps: int, eta: float):\n",
    "        # =====\n",
    "        # your code\n",
    "        # apply Langevin dynamics in for-cycle to the starting point x\n",
    "        \n",
    "        # =====\n",
    "        return x\n",
    "\n",
    "    def sample(self, num_samples: int = 64, num_steps: int=100, eta: float = 0.01):\n",
    "        with torch.no_grad():\n",
    "            # we sample x_0 from U[-1, 1]\n",
    "            x0 = 2. * torch.rand_like(torch.empty(num_samples, *self.input_shape)) - 1.\n",
    "            x0 = x0.to(self.device)\n",
    "\n",
    "            # run langevine dynamics\n",
    "            x = self.langevin_dynamics(x0, num_steps=num_steps, eta=eta)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_denoiser_score_matcher():\n",
    "    matcher = DenoisingScoreMatcher(\n",
    "        score_model=nn.Linear(2, 2),\n",
    "        input_shape=(2,),\n",
    "        sigma=0.1\n",
    "    )\n",
    "    x = torch.rand(16, 2)\n",
    "    assert x.size() == matcher(x).size()\n",
    "    loss = matcher.loss(x)[\"total_loss\"]\n",
    "    assert len(loss.size()) == 0\n",
    "    assert list(matcher.sample(4).size()) == [4, 2]\n",
    "\n",
    "\n",
    "test_denoiser_score_matcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaIv3M6crme-"
   },
   "source": [
    "That's all!\n",
    "\n",
    "And now we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958
    },
    "executionInfo": {
     "elapsed": 72153,
     "status": "ok",
     "timestamp": 1722176910929,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "ft98D6FT7FbY",
    "outputId": "004d75d1-b6d0-4e53-cc9d-b5b13fa52375"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "HIDDEN_SIZE = \n",
    "SIGMA = \n",
    "# ====\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# define sequential model\n",
    "# it is enough to use the sequence of Linear layers with activations\n",
    "score_model = \n",
    "# ====\n",
    "\n",
    "matcher = DenoisingScoreMatcher(\n",
    "    score_model=score_model, input_shape=(2,), sigma=SIGMA\n",
    ")\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# choose any optimizer/scheduler as you want\n",
    "optimizer = torch.optim.Adam(matcher.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "# ====\n",
    "\n",
    "dgm_utils.train_model(\n",
    "    matcher,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    n_samples=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLUKPrror7GJ"
   },
   "source": [
    "Let sample from our model. Experiment with number of steps and $\\eta$ for Langevin dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1722176910929,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "dQk73I-G_CVa",
    "outputId": "9ef88618-2f22-49e2-e659-cea1b097a884"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "NUM_STEPS = \n",
    "ETA = \n",
    "# ====\n",
    "\n",
    "samples = matcher.sample(num_samples=5000, num_steps=NUM_STEPS, eta=ETA).cpu()\n",
    "\n",
    "dgm_utils.visualize_2d_samples(samples, title=\"Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOXZbOX5uvsy"
   },
   "source": [
    "## Task 2: DDPM for 2D data (7 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu09aalquybh"
   },
   "source": [
    "In this part you have to implement your own diffusion model (DDPM) and apply it to 2D dataset.\n",
    "\n",
    "Let's take a look at dataset samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1722176965743,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "W_-FwyrzNvgP",
    "outputId": "e6f73cbe-650e-4aa1-8559-16446593bcb3"
   },
   "outputs": [],
   "source": [
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "dgm_utils.visualize_2d_data(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRjX0tL1u5j3"
   },
   "source": [
    "Below you see the utility function, which broadcasts tensors. Look carefully at this code, we will use it in the majority of methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1722176966189,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "k3sDebEwvOA8"
   },
   "outputs": [],
   "source": [
    "def _extract_into_tensor(arr, indices, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D torch tensor for a batch of indices.\n",
    "    :param arr: 1-D torch tensor.\n",
    "    :param timesteps: a tensor of indices to extract from arr.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    assert len(arr.shape) == 1\n",
    "    res = arr.to(device=indices.device)[indices].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "\n",
    "\n",
    "    return res.expand(broadcast_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d4Cw2PDvPd7"
   },
   "source": [
    "### Forward Diffusion\n",
    "\n",
    "Let start with forward diffusion.\n",
    "\n",
    "**Forward process** is defined as a posterior distribution $q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)$.\n",
    "\n",
    "It is a Markov chain, which consequently adds gaussian noise to a given object $\\mathbf{x}_0$.\n",
    "\n",
    "At every step of this process the gaussian noise is added with different magnitude, which is determined with a schedule of variances $\\{\\beta_1, ... \\beta_T\\}$.\n",
    "If this schedule is chosen properly and T goes to infinity (or is large enough), we will converge to pure noise $\\mathcal{N}(0, I)$.\n",
    "\n",
    "Markov chain is defined by:\n",
    "$$\n",
    " q(\\mathbf{x}_t | \\mathbf{x}_{t - 1}) = \\mathcal{N}(\\mathbf{x}_t | \\sqrt{1 - \\beta_t}\\mathbf{x}_{t - 1}, \\beta_t \\mathbf{I}), \\quad q(\\mathbf{x}_{1:T}|\\mathbf{x}_0) = \\prod_{t = 1}^T q(\\mathbf{x}_t | \\mathbf{x}_{t - 1})\n",
    "$$\n",
    "\n",
    "In order to get $\\mathbf{x}_t$ we have to compute $\\mathbf{x}_1, ..., \\mathbf{x}_{t - 1}$ iteratively.\n",
    "\n",
    "Hopefully, due to the properties of the gaussian distribution we can do it more efficiently.\n",
    "\n",
    "Let's denote\n",
    "$\\alpha_t = 1- \\beta_t$ и $\\bar{\\alpha}_t= \\prod_{s = 1}^t\\alpha_s$.\n",
    "Then\n",
    "$$\n",
    "q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t|\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1-\\bar{\\alpha}_t) \\mathbf{I}).\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Here we could get very useful expression\n",
    "$$\n",
    "    \\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t} \\cdot \\boldsymbol{\\epsilon}. \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZqz4__ivRt3"
   },
   "source": [
    "Now we will create base class for diffusion (we will use it as a python base class for forward and backward diffusions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1722176996073,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "D-P_EN82vPu0",
    "outputId": "59095e44-548b-43ea-cc6b-c8f72768eabc"
   },
   "outputs": [],
   "source": [
    "class BaseDiffusion:\n",
    "    def __init__(self, num_timesteps: int):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.betas = self._get_beta_schedule(num_timesteps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_beta_schedule(num_diffusion_timesteps, s=0.008):\n",
    "        def f(t, T):\n",
    "            return (np.cos((t / T + s) / (1 + s) * np.pi / 2)) ** 2\n",
    "\n",
    "        alphas = []\n",
    "        f0 = f(0, num_diffusion_timesteps)\n",
    "\n",
    "        for t in range(num_diffusion_timesteps + 1):\n",
    "            alphas.append(f(t, num_diffusion_timesteps) / f0)\n",
    "\n",
    "        betas = []\n",
    "\n",
    "        for t in range(1, num_diffusion_timesteps + 1):\n",
    "            betas.append(min(1 - alphas[t] / alphas[t - 1], 0.999))\n",
    "\n",
    "        return torch.from_numpy(np.array(betas)).double()\n",
    "\n",
    "\n",
    "basediff = BaseDiffusion(num_timesteps=20)\n",
    "\n",
    "plt.plot(basediff.betas.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5m8YNzEvUWY"
   },
   "source": [
    "We are ready to define forward diffusion process. It has 2 methods:\n",
    "- to get mean and variance of the distribution $q(\\mathbf{x}_t | \\mathbf{x}_0)$,\n",
    "- to get samples from this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1722176996378,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "pnTVeKpQvVYX"
   },
   "outputs": [],
   "source": [
    "class ForwardDiffusion(BaseDiffusion):\n",
    "    def get_mean_variance(self, x0, t):\n",
    "        # ====\n",
    "        # your code\n",
    "        # calculate mean and variance of the distribution q(x_t | x_0) (use equation (1))\n",
    "        # use _extract_into_tensor() function to get tensors of the same shape as x0\n",
    "        mean = \n",
    "        variance = \n",
    "        # ====\n",
    "        return mean, variance\n",
    "\n",
    "    def get_samples(self, x0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        # ====\n",
    "        # your code\n",
    "        # sample from the distribution q(x_t | x_0) (use equation (2))\n",
    "        samples = \n",
    "        # ====\n",
    "        return samples\n",
    "\n",
    "\n",
    "def test_forward_diffusion():\n",
    "    fdiff = ForwardDiffusion(num_timesteps=100)\n",
    "    SHAPE = [2, 20]\n",
    "    x0 = torch.ones(SHAPE)\n",
    "    t = torch.ones((2,)).long() * 5\n",
    "    mean, variance = fdiff.get_mean_variance(x0=x0, t=t)\n",
    "    assert list(mean.shape) == SHAPE\n",
    "    assert list(variance.shape) == SHAPE\n",
    "    assert np.allclose(mean.numpy(), np.ones(SHAPE) * 0.9944681)\n",
    "    assert np.allclose(variance.numpy(), np.ones(SHAPE) * 0.01103322)\n",
    "\n",
    "    xt = fdiff.get_samples(x0=x0, t=t)\n",
    "    assert list(xt.shape) == SHAPE\n",
    "\n",
    "    noise = torch.ones(SHAPE)\n",
    "    xt = fdiff.get_samples(x0=x0, t=t, noise=noise)\n",
    "    assert np.allclose(xt.numpy(), np.ones(SHAPE) * 1.0995072)\n",
    "\n",
    "\n",
    "test_forward_diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JvAMhzOvWst"
   },
   "source": [
    "Let visualize the forward diffusion process. Here you have to see how the distribution of the real samples transforms to the gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2615,
     "status": "ok",
     "timestamp": 1722177000521,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "ShTXHGiSvYMH",
    "outputId": "19d32efd-a59b-43db-b041-82b3f4954006"
   },
   "outputs": [],
   "source": [
    "T = 100\n",
    "\n",
    "fdiff = ForwardDiffusion(num_timesteps=T)\n",
    "\n",
    "timestamps=[0, 2, 4, 10, 50]\n",
    "\n",
    "plot_n_steps = len(timestamps)\n",
    "for i, t in enumerate(timestamps):\n",
    "    x = fdiff.get_samples(x0=torch.from_numpy(train_data), t=torch.ones((train_data.shape[0], 1)).long() * t)\n",
    "    dgm_utils.visualize_2d_samples(x, title=f\"Step of diffusion: {t}\", labels=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBwkyW6yvaBA"
   },
   "source": [
    "### Reverse Diffusion\n",
    "\n",
    "**Reverse process** consequently denoises pure gaussian noise $\\mathcal{N}(0, \\mathbf{I})$ until we do not get the object from the original distribution $\\pi(\\mathbf{x})$.\n",
    "\n",
    "It is a probability model with latent variables\n",
    "$p(\\mathbf{x}_0 | \\boldsymbol{\\theta}) := \\int p(\\mathbf{x}_{0:T} | \\boldsymbol{\\theta}) d\\mathbf{x}_{1:T}$,\n",
    "where\n",
    "- latents $\\mathbf{z} = \\{\\mathbf{x}_1, ..., \\mathbf{x}_T \\}$ correspond to noised objects\n",
    "- $\\mathbf{x}_0$ is an object from the original distribution $\\pi(\\mathbf{x})$.\n",
    "\n",
    "Joint distribution $p(\\mathbf{x}_{0:T} | \\boldsymbol{\\theta})$ is called reverse diffusion process, which is essentially a Markov chain of gaussian distributions $p(\\mathbf{x}_t|\\mathbf{x}_t, \\boldsymbol{\\theta})$:\n",
    "$$\n",
    "p(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod_{t = 1}^T p(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\boldsymbol{\\theta}), \\quad p(\\mathbf{x}_{T} | \\boldsymbol{\\theta})=\\mathcal{N}(0, \\mathbf{I})\n",
    "$$\n",
    "$$\n",
    "  p(\\mathbf{x}_{t - 1}|\\mathbf{x}_t | \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t), \\boldsymbol{\\sigma}^2_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t)). \\tag{3}\n",
    "$$\n",
    "\n",
    "In Lecture 11 we have derived ELBO for this model:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(q, \\boldsymbol{\\theta}) =  \\mathbb{E}_{q} \\Bigl[\\log p(\\mathbf{x}_0 | \\mathbf{x}_1, \\boldsymbol{\\theta}) - KL\\bigl(q(\\mathbf{x}_T | \\mathbf{x}_0) || p(\\mathbf{x}_T)\\bigr)\n",
    "    - \\sum_{t=2}^T \\underbrace{KL \\bigl(q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) || p(\\mathbf{x}_{t - 1} | \\mathbf{x}_t, \\boldsymbol{\\theta} )\\bigr)}_{\\mathcal{L}_t} \\Bigr].\n",
    "$$\n",
    "\n",
    "Here we use the following distribution $q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}( \\boldsymbol{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I}) $, where\n",
    "$$\n",
    "\\boldsymbol{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0) = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0\n",
    "\\tag{4}\n",
    "$$\n",
    "$$\n",
    "\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "(These scary formulas are not difficult to derive, follow the link to find details [Denoising Diffusion Probabilistic Models (Ho et al. 2020)](https://arxiv.org/abs/2006.11239)).\n",
    "\n",
    "Now our goal is to define parameters $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t), \\boldsymbol{\\sigma}^2_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t)$ of reverse diffusion.\n",
    "\n",
    "#### Variance\n",
    "Our first assumption is to set the variance $\\boldsymbol{\\sigma}^2_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t) = \\tilde{\\beta}_t$. This is very native assumption\n",
    "\n",
    "#### Mean\n",
    "Here we will use the expression (2) to get $\\mathbf{x}_0$ from $\\mathbf{x}_t$:\n",
    "$$\n",
    "    \\mathbf{x}_0 = \\frac{\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_{t}} \\cdot \\boldsymbol{\\epsilon}}{\\sqrt{\\bar{\\alpha}_{t}}}.\n",
    "    \\tag{6}\n",
    "$$\n",
    "\n",
    "If we put this expression to the formula (4) we will get:\n",
    "$$\n",
    "    \\boldsymbol{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\cdot \\boldsymbol{\\epsilon} \\right).\n",
    "$$\n",
    "\n",
    "So the idea here to parametrize the model mean in the same functional form:\n",
    "$$\n",
    "    \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\cdot \\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t) \\right).\n",
    "$$\n",
    "\n",
    "**Note:** our model will predict the noise which was applied to $\\mathbf{x}_0$ to get $\\mathbf{x}_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1722177003542,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "tuFLezfZvbkV"
   },
   "outputs": [],
   "source": [
    "class ReverseDiffusion(BaseDiffusion):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alphas_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=self.betas.device), self.alphas_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # calculate variance of the distribution q(x_{t-1} | x_t, x_0) (use equation (5))\n",
    "        self.variance = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # calculate coefficients of mean of the distribution q(x_{t-1} | x_t, x_0) (use equation (4))\n",
    "        # mean = x_coef * x_t + x0_coef * x_0\n",
    "        self.xt_coef = \n",
    "        self.x0_coef = \n",
    "        # ====\n",
    "\n",
    "    def get_x0(self, xt, eps, t):\n",
    "        # ====\n",
    "        # your code\n",
    "        # get x_0 (use equation (6))\n",
    "        x0 = \n",
    "        # ====\n",
    "        return x0\n",
    "\n",
    "    def get_mean_variance(self, xt, eps, t):\n",
    "        # ====\n",
    "        # your code\n",
    "        # get mean and variance of the distribution q(x_{t-1} | x_t, x_0) (use equations (4) and (5))\n",
    "        # use get_x0 method to get x_0\n",
    "        variance = \n",
    "        mean = \n",
    "        # ====\n",
    "        return mean, variance\n",
    "\n",
    "    def get_samples(self, xt, eps, t):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get mean and variance of the distribution q(x_{t-1} | x_t, x_0)\n",
    "        # 2) sample noise from the standard normal\n",
    "        # 3) get samples using reparametrization trick\n",
    "        sample =\n",
    "        # ====\n",
    "        return sample.float()\n",
    "\n",
    "\n",
    "def test_reverse_diffusion():\n",
    "    rdiff = ReverseDiffusion(num_timesteps=100)\n",
    "    SHAPE = [2, 20]\n",
    "    xt = torch.ones(SHAPE)\n",
    "    eps = torch.ones(SHAPE)\n",
    "    t = torch.ones((2,)).long() * 5\n",
    "\n",
    "    x0 = rdiff.get_x0(xt=xt, eps=eps, t=t)\n",
    "    assert list(x0.shape) == SHAPE\n",
    "    assert np.allclose(x0.numpy(), np.ones(SHAPE) * 0.8999391)\n",
    "\n",
    "    mean, variance = rdiff.get_mean_variance(xt=xt, eps=eps, t=t)\n",
    "    assert list(mean.shape) == SHAPE\n",
    "    assert list(variance.shape) == SHAPE\n",
    "    assert np.allclose(mean.numpy(), np.ones(SHAPE) * 0.9723116)\n",
    "    assert np.allclose(variance.numpy(), np.ones(SHAPE) * 0.00222036)\n",
    "\n",
    "    x = rdiff.get_samples(xt, eps, t)\n",
    "    assert list(x.shape) == SHAPE\n",
    "\n",
    "\n",
    "test_reverse_diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kGefGAtvdH7"
   },
   "source": [
    "### Model\n",
    "\n",
    "In this task we will use simple MLP model to parametrize distribution $p(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\boldsymbol{\\theta})$. It will be conditioned on the timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1722177005248,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "-aOpVMa1veXD"
   },
   "outputs": [],
   "source": [
    "class ConditionalMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_embeds: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.x_proj = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.t_proj = nn.Embedding(num_embeds, self.hidden_dim)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(self.hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.x_proj(x)\n",
    "        t = self.t_proj(t.int())\n",
    "        x = x + t\n",
    "        x = F.selu(x)\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "def test_conditional_mlp():\n",
    "    SHAPE = [2, 20]\n",
    "    T = 100\n",
    "    x = torch.ones(SHAPE)\n",
    "    t = torch.ones((2,)).long() * 5\n",
    "    model = ConditionalMLP(input_dim=20, num_embeds=100)\n",
    "    output = model(x, t)\n",
    "    assert list(output.shape) == SHAPE\n",
    "\n",
    "\n",
    "test_conditional_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idlTvGv5vgOs"
   },
   "source": [
    "### DDPM\n",
    "\n",
    "Let return to the ELBO. The main part of it is:\n",
    "$$\n",
    "    \\mathcal{L}_t = KL \\bigl(q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) || p(\\mathbf{x}_{t - 1} | \\mathbf{x}_t, \\boldsymbol{\\theta} )\\bigr)\n",
    "$$\n",
    "\n",
    "In Lecture 11 we have got that\n",
    "$$\n",
    "    \\mathcal{L}_t = \\mathbb{E}_{\\boldsymbol{\\epsilon}} \\left[ \\frac{\\beta_t^2}{2 \\tilde{\\beta_t} \\alpha_t (1 - \\bar{\\alpha}_t)} \\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t) \\|^2 \\right].\n",
    "$$\n",
    "\n",
    "In practice this loss is simplified. Particilarly, we will omit coefficient of the norm and we will sample index $t$ at each training step.\n",
    "\n",
    "Finally, we will train our model with the following objective:\n",
    "$$\n",
    "\\text{loss} = \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}, t}\\bigg[ \\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t)\\|^2\\bigg],\n",
    "$$\n",
    "where $\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndI8JG0Avj3B"
   },
   "source": [
    "The following class implements two methods:\n",
    "- `loss` - to compute the loss at the training step;\n",
    "- `sample` - to sample from the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1722177011535,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "H2o0OMpyvhUb"
   },
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(self, num_timesteps: int, model: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        self.forward_diffusion = ForwardDiffusion(num_timesteps=num_timesteps)\n",
    "        self.reverse_diffusion = ReverseDiffusion(num_timesteps=num_timesteps)\n",
    "        self.model = model\n",
    "        self.shape = None\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "\n",
    "    def sample(self, num_samples: int):\n",
    "        assert self.shape is not None\n",
    "        x = torch.randn((num_samples, *self.shape), device=self.device, dtype=torch.float32)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        for i in tqdm(indices):\n",
    "            t = torch.tensor([i] * num_samples, device=x.device)\n",
    "            with torch.no_grad():\n",
    "                # ====\n",
    "                # your code\n",
    "                # 1) get epsilon from the model\n",
    "                # 2) sample from the reverse diffusion\n",
    "                x = \n",
    "                # ====\n",
    "        return x\n",
    "\n",
    "    def loss(self, x0):\n",
    "        if self.shape is None:\n",
    "            self.shape = list(x0.shape)[1:]\n",
    "        t = torch.randint(0, self.num_timesteps, size=(x0.size(0),), device=x0.device)\n",
    "        noise = torch.randn_like(x0)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get x_t\n",
    "        # 2) get epsilon from the model\n",
    "        # 3) compute mse loss between epsilon and noise\n",
    "        eps = \n",
    "        # ====\n",
    "        loss = F.mse_loss(eps, noise)\n",
    "        return {\"total_loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh3ubgScvmZd"
   },
   "source": [
    "### Training\n",
    "\n",
    "Now we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 46302,
     "status": "ok",
     "timestamp": 1722177058890,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "Y16rD_Zivnty",
    "outputId": "79577186-0c94-4dfd-ade1-2455b6989c8c"
   },
   "outputs": [],
   "source": [
    "T = 100 # you could change it\n",
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "LR = \n",
    "EPOCHS = \n",
    "# ====\n",
    "\n",
    "model = ConditionalMLP(input_dim=2, num_embeds=T)\n",
    "ddpm = DDPM(num_timesteps=T, model=model)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# try your own optimizer/scheduler\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=LR, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.995)\n",
    "\n",
    "dgm_utils.train_model(\n",
    "    model=ddpm,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    visualize_samples=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWUI-7U5vph-"
   },
   "source": [
    "Now let's sample from our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1722177059680,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "c2E5mdxovqoB",
    "outputId": "805726f2-1eac-4599-8d7f-7be8ed93b9fa"
   },
   "outputs": [],
   "source": [
    "samples = ddpm.sample(num_samples=5000).cpu()\n",
    "\n",
    "dgm_utils.visualize_2d_samples(samples, title=\"Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F3hzwYovr5S"
   },
   "source": [
    "Now let's see how denoising looks like (similarly to forward noising process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1388,
     "status": "ok",
     "timestamp": 1722177061060,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "_cs-9SgRvtSz",
    "outputId": "ae37db02-5d49-4edb-ae55-1c7fb6a79670"
   },
   "outputs": [],
   "source": [
    "timestamps=[0, 2, 4, 10, 50]\n",
    "\n",
    "x = torch.randn(train_data.shape[0], 2, requires_grad=False).to(ddpm.device)\n",
    "for i in range(ddpm.num_timesteps - 1, -1, -1):\n",
    "    t = torch.tensor(i, dtype=torch.long, requires_grad=False).expand(x.shape[0]).to(ddpm.device)\n",
    "    with torch.no_grad():\n",
    "        eps = ddpm.model(x, t)\n",
    "        x = ddpm.reverse_diffusion.get_samples(xt=x, eps=eps, t=t)\n",
    "    if i in reversed(timestamps):\n",
    "        x_ = x.cpu()\n",
    "        dgm_utils.visualize_2d_samples(x_, title=f\"Samples from timestamp: {i}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
